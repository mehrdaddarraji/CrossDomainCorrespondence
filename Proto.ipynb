{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6ff74b3c3bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V\n",
    "from torchvision.models import vgg19\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype File for model algorithm\n",
    "<img src=\"algorithm_high_level.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mehrdad is able to get us the features (preprocessing) using this code\n",
    "class vgg19_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg19_model, self).__init__()\n",
    "        self.model = vgg19(pretrained=True)\n",
    "        # self.layers = self.forward(model)\n",
    "        \n",
    "#         self.batchSize = 1\n",
    "#         self.num_channel = 3\n",
    "#         self.img_size = 224\n",
    "#         self.input = torch.Tensor(self.batchSize, self.num_channel, self.img_size, self.img_size)\n",
    "        # print(self.layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        pyramid_layers = []\n",
    "        def extract_feature(self, input, output):\n",
    "            pyramid_layers.append(output)\n",
    "            \n",
    "        relu_idx = [1, 6, 11, 20, 29]\n",
    "        for i in relu_idx:\n",
    "            self.model.features[i].register_forward_hook(extract_feature)\n",
    "        \n",
    "        # Image preprocessing\n",
    "        # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] \n",
    "        # and std=[0.229, 0.224, 0.225].\n",
    "        # We use the same normalization statistics here.\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                 std=(0.229, 0.224, 0.225))])\n",
    "        \n",
    "        img_preproc = transform(img)\n",
    "        img_preproc = torch.unsqueeze(img_preproc, 0)\n",
    "        self.model(V(img_preproc))\n",
    "        \n",
    "        return pyramid_layers\n",
    "\n",
    "# vgg = vgg19_model()\n",
    "# im_a = Image.open(\"../input/dog1.jpg\")\n",
    "# im_a_feature = vgg.forward(im_a)\n",
    "# im_a_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer in range(5, 1):\n",
    "    # extract A sub l, (candidate NNBs computed for the lth level)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
