{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6ff74b3c3bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V\n",
    "from torchvision.models import vgg19\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg19_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg19_model, self).__init__()\n",
    "        self.model = vgg19(pretrained=True)\n",
    "        # self.layers = self.forward(model)\n",
    "        \n",
    "#         self.batchSize = 1\n",
    "#         self.num_channel = 3\n",
    "#         self.img_size = 224\n",
    "#         self.input = torch.Tensor(self.batchSize, self.num_channel, self.img_size, self.img_size)\n",
    "        # print(self.layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        pyramid_layers = []\n",
    "        def extract_feature(self, input, output):\n",
    "            pyramid_layers.append(output)\n",
    "            \n",
    "        relu_idx = [1, 6, 11, 20, 29]\n",
    "        for i in relu_idx:\n",
    "            self.model.features[i].register_forward_hook(extract_feature)\n",
    "        \n",
    "        # Image preprocessing\n",
    "        # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] \n",
    "        # and std=[0.229, 0.224, 0.225].\n",
    "        # We use the same normalization statistics here.\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                 std=(0.229, 0.224, 0.225))])\n",
    "        \n",
    "        img_preproc = transform(img)\n",
    "        img_preproc = torch.unsqueeze(img_preproc, 0)\n",
    "        self.model(V(img_preproc))\n",
    "        \n",
    "        return pyramid_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg19_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cd6f861a3770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg19_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mim_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/dog1.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mim_a_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# im_a_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vgg19_model' is not defined"
     ]
    }
   ],
   "source": [
    "vgg = vgg19_model()\n",
    "im_a = Image.open(\"../input/dog1.jpg\")\n",
    "im_a_feature = vgg.forward(im_a)\n",
    "# im_a_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __repr__(self):\n",
    "        return \"(\" + str(self.x) + \", \" + str(self.y) + \")\"\n",
    "    def __lt__(self, other):\n",
    "        return self.x < other.x\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "    def __gt__(self, other):\n",
    "        return self.x > other.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RefineSearchRegions(prev_layer_nbbs, receptive_field_radius, feat_width, feat_height):\n",
    "        \"\"\"\n",
    "        Return refined search regions for every p and q in the previous' layer nbbs\n",
    "\n",
    "        Args:\n",
    "            prev_layer_nbbs: Previous' layer (l-1) neural best buddies, represented\n",
    "                as neurons using the Neuron class\n",
    "            receptive_field_radius: radius of new search regions\n",
    "                equal to 4 for l = 2,3 and equal to 6 for l = 4, 5\n",
    "            feat_width: width of feature map for current layer\n",
    "            feat_height: height of feature map for current layer\n",
    "        Returns:\n",
    "            Ps: List containing new P's \n",
    "                P = ((r1, c1), (r2, c2))\n",
    "                where (r1, c1) represent the top left of the search region\n",
    "                and (r2, c2) represent the bottom right of the search region\n",
    "            Qs: List containing new Q's\n",
    "                Q = ((r1, c1), (r2, c2))\n",
    "                where (r1, c1) represent the top left of the search region\n",
    "                and (r2, c2) represent the bottom right of the search region\n",
    "        \"\"\"\n",
    "        \n",
    "        Ps = []\n",
    "        Qs = []\n",
    "        \n",
    "        for p, q in prev_layer_nbbs:\n",
    "           \n",
    "            # Top left of search window for P\n",
    "            P_r1 = max(2 * p.r - receptive_field_radius / 2, 0)\n",
    "            P_c2 = max(2 * p.c - receptive_field_radius / 2, 0)\n",
    "            P_bottom_left = Neuron(P_r1, P_c1)\n",
    "                      \n",
    "            # Bottom right of search window for P\n",
    "            P_r2 = min(2 * p.r + receptive_field_radius / 2, feat_width)\n",
    "            P_c2 = min(2 * p.c + receptive_field_radius / 2, feat_height)\n",
    "            P_top_right = Neuron(P_r2, P_c2)\n",
    "            \n",
    "            # Top left of search window for Q\n",
    "            Q_r1 = max(2 * q.r - receptive_field_radius / 2, 0)\n",
    "            Q_c1 = max(2 * q.c - receptive_field_radius / 2, 0)\n",
    "            Q_bottom_left = Neuron(Q_c1, Q_r1)\n",
    "            \n",
    "            # Bottom right of search window for Q                     \n",
    "            Q_r2 = min(2 * q.r + receptive_field_radius / 2, feat_width)\n",
    "            Q_c2 = min(2 * q.c + receptive_field_radius / 2, feat_height)\n",
    "            Q_top_right = Neuron(Q_c2, Q_r2)\n",
    "            \n",
    "            # Append P and Q to lists\n",
    "            Ps.append((P_bottom_left, P_top_right))\n",
    "            Qs.append((Q_bottom_left, Q_top_right))\n",
    "        \n",
    "        return (Ps, Qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
